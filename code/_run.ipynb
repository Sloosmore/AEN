{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (4.37.0.dev0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.9.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Function to check if a package is installed\n",
    "def install_and_import(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of packages to check and install if necessary\n",
    "packages = [\n",
    "    'torchmetrics',\n",
    "    'sentence-transformers',\n",
    "    'transformers',\n",
    "    'gspread',\n",
    "    'oauth2client',\n",
    "    'dask'\n",
    "]\n",
    "\n",
    "# Checking and installing each package\n",
    "for package in packages:\n",
    "    install_and_import(package)\n",
    "\n",
    "# Now you can import all necessary modules\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torchmetrics.classification import BinaryF1Score, BinaryAccuracy, BinaryPrecision, BinaryRecall\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-aware KDE plots have been generated and displayed.\n"
     ]
    }
   ],
   "source": [
    "from model_def import *\n",
    "from custom_data_set import *\n",
    "from train_script import *\n",
    "from misc import *\n",
    "from save_load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be pulling from checkpoint model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Path             CP2_1\n",
       "Complete         FALSE\n",
       "Epochs               1\n",
       "Train Loss            \n",
       "Test Loss             \n",
       "Train F1              \n",
       "Test F1               \n",
       "Cost per hour         \n",
       "Time (Hours)          \n",
       "Date                  \n",
       "Notes                 \n",
       "Performance           \n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_hyperparameters():\n",
    "    hyperparameters = {\n",
    "        'Encoder': 'bert-base-uncased',\n",
    "        'FFNN': 'True',\n",
    "        'Same Eno': 'True',\n",
    "        'Encoder Req Grad': 'False',\n",
    "        'Freeze Epochs': '0',\n",
    "        'Bandwith': '0.5',\n",
    "        'PDF': 'gaussian',\n",
    "        'Batch Size': '32',\n",
    "        'Epochs': '10',\n",
    "        'loss weight': '1.0',\n",
    "        'lr': '0.001',\n",
    "        'L2': '0.0'\n",
    "    }\n",
    "    return hyperparameters\n",
    "\n",
    "hyperparameters = get_hyperparameters()\n",
    "cp = hyperparameters.copy()\n",
    "cp_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = (hyperparameters['Encoder'])\n",
    "model_config = AutoConfig.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "embedding_dim = model_config.hidden_size\n",
    "end_pred_sequential = eval(hyperparameters['FFNN'])\n",
    "same_encoder = True if hyperparameters['Same Eno'].lower() == \"true\" else False\n",
    "encoder_grad = True if hyperparameters['Encoder Req Grad'].lower() == \"true\" else False\n",
    "freeze_epochs_value = hyperparameters['Freeze Epochs']\n",
    "vect_settings = {'apply_to': hyperparameters['Freeze Epochs'], 'bandwidth': str(hyperparameters['Bandwith']), 'pdf_type': hyperparameters['PDF']}\n",
    "\n",
    "try:\n",
    "    freeze_epochs = int(freeze_epochs_value)\n",
    "except (ValueError, TypeError):\n",
    "    freeze_epochs = eval(freeze_epochs_value) if isinstance(freeze_epochs_value, str) else freeze_epochs_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning done, now splitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lengths: 100%|██████████| 47/47 [00:01<00:00, 39.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47916 samples, train=True\n",
      "Cleaning done, now splitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating lengths: 100%|██████████| 2/2 [00:00<00:00, 67.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1229 samples, train=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "data_path = [\"../ data/2.5m.csv\"]\n",
    "batch_size = int(hyperparameters['Batch Size'])\n",
    "model_config = AutoConfig.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "test_size = .025\n",
    "collate = Collate_Fn()\n",
    "\n",
    "train_dataset = Load_Py_CSV(data_path, tokenizer=tokenizer, test_size=test_size)\n",
    "test_dataset  = Load_Py_CSV(data_path, tokenizer=tokenizer, test_size=test_size, train=False)\n",
    "\n",
    "train_sampler = BatchLenSampler(data_source=train_dataset, batch_size=batch_size, seed=random_seed)\n",
    "test_sampler = BatchLenSampler(data_source=test_dataset, batch_size=batch_size, seed=random_seed)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, collate_fn=collate)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, collate_fn=collate)\n",
    "\n",
    "# Iterate over the dataloader to get a random batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(s) trainable status set to: True\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model = TSN_model(\n",
    "    encoder_model=model_path,\n",
    "    device=device,\n",
    "    same_encoder=same_encoder,\n",
    "    encoder_tune=encoder_grad,\n",
    "    end_pred_sequential=end_pred_sequential,\n",
    "    kde_config=vect_settings\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint\n",
      "Model loaded from ../../saved_models/checkpoints/CP1_16.pt\n",
      "Resuming from epoch 1\n"
     ]
    }
   ],
   "source": [
    "Epochs = cp['Epochs'] if checkpoint_mode else hyperparameters['Epochs']\n",
    "\n",
    "metrics_measure = [BinaryF1Score(threshold=.5).to(device), BinaryPrecision(threshold=.5).to(device), BinaryRecall(threshold=.5).to(device)]\n",
    "\n",
    "bi_ratio_weight = torch.Tensor([float(hyperparameters[\"loss weight\"])])\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=bi_ratio_weight).to(device)\n",
    "\n",
    "lr = float(hyperparameters['lr'])\n",
    "\n",
    "param_list = [{\"params\": model.prompt_encoder.parameters(), \"weight_decay\":0},\n",
    "               {\"params\": model.caption_encoder.parameters(), \"weight_decay\":0},\n",
    "               {\"params\": model.pred_layer.parameters(), \"weight_decay\":float(hyperparameters['L2'])},]\n",
    "\n",
    "if cp_path:\n",
    "    print(\"Loading model from checkpoint\")\n",
    "    model, optim_fn, start_epoch, loaded_metrics = load_model(filepath=cp_path,\n",
    "                                                 model=model,\n",
    "                                                 device=device)\n",
    "else:\n",
    "    optim_fn = None\n",
    "    start_epoch = 0\n",
    "    \n",
    "optim_fn = optim_fn or optim.Adam(param_list, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 0.858 |BinaryF1Score: 0.285, BinaryPrecision: 0.220, BinaryRecall: 0.406 | Test Loss: 0.830 | BinaryF1Score: 0.339, BinaryPrecision: 0.265, BinaryRecall: 0.480\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "new_metrics = train_model(model=model, \n",
    "                      tokenizer=tokenizer, \n",
    "                      optim=optim_fn, \n",
    "                      loss_fn=loss_fn, \n",
    "                      train_data=train_dataloader, \n",
    "                      test_data=test_dataloader, \n",
    "                      metrics=metrics_measure, \n",
    "                      epochs=Epochs, \n",
    "                      device=device,\n",
    "                      freeze_epochs=freeze_epochs, \n",
    "                      start_epochs=start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss_ar': [0.887244701385498],\n",
       " 'train_metrics_ar': [tensor(0.2598, device='mps:0'),\n",
       "  tensor(0.2069, device='mps:0'),\n",
       "  tensor(0.3792, device='mps:0')],\n",
       " 'test_loss_ar': [0.8447503447532654],\n",
       " 'test_metrics_ar': [tensor(0.3331, device='mps:0'),\n",
       "  tensor(0.2718, device='mps:0'),\n",
       "  tensor(0.4339, device='mps:0')],\n",
       " 'time_elapsed': 105.9577488899231}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cp_path and loaded_metrics:\n",
    "    combined_metrics = {\n",
    "        \"train_loss_ar\": loaded_metrics[\"train_loss_ar\"] + new_metrics[\"train_loss_ar\"],\n",
    "        \"test_loss_ar\": loaded_metrics[\"test_loss_ar\"] + new_metrics[\"test_loss_ar\"],\n",
    "        \"time_elapsed\": loaded_metrics[\"time_elapsed\"] + new_metrics[\"time_elapsed\"]\n",
    "    }\n",
    "    \n",
    "    # Function to safely concatenate tensors or convert scalars to tensors\n",
    "    def safe_cat(t1, t2):\n",
    "        if t1.dim() == 0:\n",
    "            t1 = t1.unsqueeze(0)\n",
    "        if t2.dim() == 0:\n",
    "            t2 = t2.unsqueeze(0)\n",
    "        return torch.cat([t1, t2])\n",
    "    \n",
    "    # Combine train and test metrics\n",
    "    combined_metrics[\"train_metrics_ar\"] = [\n",
    "        safe_cat(loaded_metrics[\"train_metrics_ar\"][i], new_metrics[\"train_metrics_ar\"][i])\n",
    "        for i in range(len(loaded_metrics[\"train_metrics_ar\"]))\n",
    "    ]\n",
    "    combined_metrics[\"test_metrics_ar\"] = [\n",
    "        safe_cat(loaded_metrics[\"test_metrics_ar\"][i], new_metrics[\"test_metrics_ar\"][i])\n",
    "        for i in range(len(loaded_metrics[\"test_metrics_ar\"]))\n",
    "    ]\n",
    "else:\n",
    "    combined_metrics = new_metrics\n",
    "\n",
    "train_loss_ar, test_loss_ar = combined_metrics[\"train_loss_ar\"], combined_metrics[\"test_loss_ar\"]\n",
    "time_elapsed = combined_metrics['time_elapsed']\n",
    "\n",
    "# Decompose metrics\n",
    "metric_names = ['F1 Score', 'Precision', 'Recall']\n",
    "train_metrics = [tensor.cpu().numpy() for tensor in combined_metrics[\"train_metrics_ar\"]]\n",
    "test_metrics = [tensor.cpu().numpy() for tensor in combined_metrics[\"test_metrics_ar\"]]\n",
    "\n",
    "# Create metrics array for plotting\n",
    "metrics_array = np.array([\n",
    "    [train_metrics[i], test_metrics[i]] for i in range(len(train_metrics))\n",
    "])\n",
    "\n",
    "# Create metrics array for plotting\n",
    "metrics_ar = [{\"name\": 'Loss', \"test_data\": test_loss_ar, \"train_data\": train_loss_ar}] + [\n",
    "    {\"name\": name, \"test_data\": test, \"train_data\": train}\n",
    "    for name, test, train in zip(metric_names, test_metrics, train_metrics)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Model Metrics')\n",
    "axs = axs.ravel()\n",
    "\n",
    "for ax, diction in zip(axs, metrics_ar):\n",
    "    ax.set_title(diction[\"name\"] + ' over time')\n",
    "    ax.set_ylabel(diction[\"name\"])\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.plot(diction[\"test_data\"], label='test')\n",
    "    ax.plot(diction[\"train_data\"], label='train')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_round(value, decimals=3):\n",
    "    \"\"\"\n",
    "    Safely round a value whether it's a tensor, numpy array, or a regular number.\n",
    "    \n",
    "    Args:\n",
    "    value: The value to round. Can be a tensor, numpy array, or a regular number.\n",
    "    decimals: The number of decimal places to round to. Default is 3.\n",
    "    \n",
    "    Returns:\n",
    "    The rounded value as a Python float.\n",
    "    \"\"\"\n",
    "    if isinstance(value, (np.ndarray, list)):\n",
    "        # If it's an array or list, round the last element\n",
    "        return round(float(value[-1]), decimals)\n",
    "    try:\n",
    "        # Try to use .item() method (for tensors)\n",
    "        return round(value.item(), decimals)\n",
    "    except AttributeError:\n",
    "        # If .item() method is not available, it's likely a regular number\n",
    "        return round(value, decimals)\n",
    "    \n",
    "    \n",
    "params = hyperparameters\n",
    "\n",
    "params['Train Loss'] = safe_round(train_loss_ar[-1], 3)\n",
    "params['Test Loss'] = safe_round(test_loss_ar[-1], 3)\n",
    "params['Train F1'] = safe_round(metrics_ar[1][\"train_data\"],3) \n",
    "params['Test F1'] = safe_round(metrics_ar[1][\"test_data\"], 3)\n",
    "params['Time (Hours)'] = safe_round(time_elapsed)/3600\n",
    "params['Cost'] = 0\n",
    "params['Date'] = current_date_with_ordinal()\n",
    "params['Complete'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../../saved_models/checkpoints already exists.\n",
      "Model saved to ../../saved_models/checkpoints/CP2_1.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = \"../checkpoints/\"\n",
    "\n",
    "create_path_if_not_exist(save_path)\n",
    "full_path =  save_path + f\"{cp['Path']}.pt\"\n",
    "save_model(model=model, \n",
    "           optimizer=optim_fn,\n",
    "           epoch=start_epoch + Epochs,\n",
    "           metrics=combined_metrics,\n",
    "           filepath=full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
